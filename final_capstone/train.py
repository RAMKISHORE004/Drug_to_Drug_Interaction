# -*- coding: utf-8 -*-
"""app_py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YpPfxsT_GvyhmZ3LiKCNojnYDgfcEih1
"""

import re
import random
import torch
import torch.nn.functional as F
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from torch_geometric.data import Data
from torch_geometric.nn import GATConv

CSV_PATH       = "DDICorpus2013.csv"
TFIDF_DIM      = 100
HIDDEN_DIM     = 64
GAT_OUT_DIM    = 128
GAT_HEADS      = 4
BATCH_SIZE     = 256
LR             = 1e-3
NUM_EPOCHS     = 20
DEVICE         = torch.device("cuda" if torch.cuda.is_available() else "cpu")

df = (
    pd.read_csv(CSV_PATH)
      .drop_duplicates()
      .reset_index(drop=True)
)

def clean_drug(s):
    return re.sub(r'[^a-z0-9 ]', '', str(s).lower().strip())

df['Drug_1'] = df['Drug_1_Name'].apply(clean_drug)
df['Drug_2'] = df['Drug_2_Name'].apply(clean_drug)
df['Sentence_Text'] = df['Sentence_Text'].fillna("")

# build drug↔index map
all_drugs = sorted(set(df['Drug_1']) | set(df['Drug_2']))
drug_to_idx = {d: i for i, d in enumerate(all_drugs)}

num_nodes = len(drug_to_idx)
x = torch.eye(num_nodes, dtype=torch.float)
edge_list = []
for d1, d2 in zip(df['Drug_1'], df['Drug_2']):
    i1, i2 = drug_to_idx[d1], drug_to_idx[d2]
    edge_list += [[i1, i2], [i2, i1]]
edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
data = Data(x=x, edge_index=edge_index).to(DEVICE)

tfidf = TfidfVectorizer(max_features=TFIDF_DIM)
text_feats = tfidf.fit_transform(df['Sentence_Text']).toarray()

positive = [
    (drug_to_idx[r['Drug_1']], drug_to_idx[r['Drug_2']], 1, text_feats[i])
    for i, r in df.iterrows()
]
all_pairs = {(i, j) for i in range(num_nodes) for j in range(i + 1, num_nodes)}
pos_pairs = {(i, j) for i, j, _, _ in positive}
neg_cands = list(all_pairs - pos_pairs)
random.seed(42)
negative = [(i, j, 0, [0]*TFIDF_DIM) for i, j in random.sample(neg_cands, len(positive))]

edges = positive + negative
train_edges, val_edges = train_test_split(edges, test_size=0.2, random_state=42)

class EdgeDataset(Dataset):
    def __init__(self, edges): self.edges = edges
    def __len__(self):       return len(self.edges)
    def __getitem__(self, idx):
        i1, i2, lbl, txt = self.edges[idx]
        return {
            "i1": torch.tensor(i1, dtype=torch.long),
            "i2": torch.tensor(i2, dtype=torch.long),
            "text": torch.tensor(txt, dtype=torch.float),
            "label": torch.tensor(lbl, dtype=torch.float)
        }

train_loader = DataLoader(EdgeDataset(train_edges), batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(EdgeDataset(val_edges),   batch_size=BATCH_SIZE)

class GATNet(torch.nn.Module):
    def __init__(self, in_c, out_c, heads=4):
        super().__init__()
        self.conv1 = GATConv(in_c, out_c, heads=heads, concat=True)
        self.conv2 = GATConv(out_c*heads, out_c, heads=1, concat=False)
    def forward(self, x, edge_index):
        x = F.elu(self.conv1(x, edge_index))
        return self.conv2(x, edge_index)

class EdgeClassifierWithText(torch.nn.Module):
    def __init__(self, emb_dim, txt_dim, hidden_dim=64):
        super().__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(2*emb_dim + txt_dim, hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, 1),
            torch.nn.Sigmoid()
        )
    def forward(self, e1, e2, t):
        return self.net(torch.cat([e1, e2, t], dim=1))

# instantiate
gat = GATNet(in_c=num_nodes, out_c=GAT_OUT_DIM, heads=GAT_HEADS).to(DEVICE)
edge_clf = EdgeClassifierWithText(emb_dim=GAT_OUT_DIM, txt_dim=TFIDF_DIM, hidden_dim=HIDDEN_DIM).to(DEVICE)
optimizer = torch.optim.Adam(list(gat.parameters()) + list(edge_clf.parameters()), lr=LR)
criterion = torch.nn.BCELoss()

# best_val_loss = float('inf')

# for epoch in range(1, NUM_EPOCHS+1):
#     gat.train(); edge_clf.train()
#     total_loss = 0

#     for batch in train_loader:
#         optimizer.zero_grad()
#         # Recompute node embeddings for each batch to avoid reuse of freed graph
#         node_emb = gat(data.x, data.edge_index)

#         emb1 = node_emb[batch["i1"]]
#         emb2 = node_emb[batch["i2"]]
#         text_feat = batch["text"].to(DEVICE)
#         labels = batch["label"].to(DEVICE).unsqueeze(1)

#         preds = edge_clf(emb1, emb2, text_feat)
#         loss = criterion(preds, labels)
#         loss.backward()
#         optimizer.step()

#         total_loss += loss.item() * len(labels)

#     avg_train = total_loss / len(train_loader.dataset)

#     # Validation
#     gat.eval(); edge_clf.eval()
#     val_loss = 0
#     with torch.no_grad():
#         # We can compute node_emb once here since no backward
#         node_emb = gat(data.x, data.edge_index)

#         for batch in val_loader:
#             emb1 = node_emb[batch["i1"]]
#             emb2 = node_emb[batch["i2"]]
#             text_feat = batch["text"].to(DEVICE)
#             labels = batch["label"].to(DEVICE).unsqueeze(1)

#             preds = edge_clf(emb1, emb2, text_feat)
#             val_loss += criterion(preds, labels).item() * len(labels)

#     avg_val = val_loss / len(val_loader.dataset)
#     print(f"Epoch {epoch:02d} — train_loss: {avg_train:.4f}, val_loss: {avg_val:.4f}")

#     if avg_val < best_val_loss:
#         best_val_loss = avg_val
#         torch.save(gat.state_dict(), "gat_best.pth")
#         torch.save(edge_clf.state_dict(), "edge_best.pth")

# print("Training complete. Best val loss:", best_val_loss)

train_losses, val_losses = [], []
train_accs,    val_accs   = [], []

best_val_acc = 0.0

for epoch in range(1, NUM_EPOCHS + 1):
    # -- Train --
    gat.train(); edge_clf.train()
    epoch_loss = 0
    correct, total = 0, 0

    for batch in train_loader:
        optimizer.zero_grad()
        # recompute embeddings each batch
        node_emb = gat(data.x, data.edge_index)

        emb1 = node_emb[batch["i1"]]
        emb2 = node_emb[batch["i2"]]
        text_feat = batch["text"].to(DEVICE)
        labels = batch["label"].to(DEVICE).unsqueeze(1)

        preds = edge_clf(emb1, emb2, text_feat)
        loss = criterion(preds, labels)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * len(labels)

        # accuracy
        predicted = (preds >= 0.5).float()
        correct += (predicted == labels).sum().item()
        total   += labels.size(0)

    train_loss = epoch_loss / total
    train_acc  = correct / total

    # -- Validate --
    gat.eval(); edge_clf.eval()
    val_loss = 0
    correct, total = 0, 0

    with torch.no_grad():
        node_emb = gat(data.x, data.edge_index)
        for batch in val_loader:
            emb1 = node_emb[batch["i1"]]
            emb2 = node_emb[batch["i2"]]
            text_feat = batch["text"].to(DEVICE)
            labels = batch["label"].to(DEVICE).unsqueeze(1)

            preds = edge_clf(emb1, emb2, text_feat)
            val_loss += criterion(preds, labels).item() * len(labels)

            predicted = (preds >= 0.5).float()
            correct += (predicted == labels).sum().item()
            total   += labels.size(0)

    val_loss /= total
    val_acc  = correct / total

    # record & possibly save
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accs.append(train_acc)
    val_accs.append(val_acc)

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(gat.state_dict(),      "gat_best.pth")
        torch.save(edge_clf.state_dict(), "edge_best.pth")

    print(f"Epoch {epoch:02d}  "
          f"Train loss: {train_loss:.4f}, acc: {train_acc:.4f}  |  "
          f"Val loss:   {val_loss:.4f}, acc: {val_acc:.4f}")

print(f"\nBest validation accuracy: {best_val_acc:.4f}")

# ----- 8. Plotting -----
epochs = range(1, NUM_EPOCHS + 1)

import matplotlib.pyplot as plt
plt.figure()
plt.plot(epochs, train_losses, label="Train Loss")
plt.plot(epochs, val_losses,   label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Loss Curve")
plt.show()

plt.figure()
plt.plot(epochs, train_accs, label="Train Acc")
plt.plot(epochs, val_accs,   label="Val Acc")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Accuracy Curve")
plt.show()